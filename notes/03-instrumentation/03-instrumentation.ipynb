{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest payoffs you will get from Prometheus are through instrumenting your own applications using direct instrumentation and client library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_http_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_http_server.py\n",
    "import http.server\n",
    "from prometheus_client import start_http_server\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Counters are the type of metric you will probably use in instrumentation most often.\n",
    "- Counters track either the number of size of events. They are mainly used to track how often a particular code path is executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_http_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_http_server.py\n",
    "import http.server\n",
    "from prometheus_client import start_http_server, Counter\n",
    "\n",
    "# Track the number of Hello Worlds returned\n",
    "REQUESTS = Counter('hello_worlds_total', 'Hello Worlds requested.')\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def do_GET(self):\n",
    "        REQUESTS.inc()\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the program, the new metric will appear on the /metrics. It will start at\n",
    "zero and increase by one2 every time you view the main URL of the application. You\n",
    "can view this in the expression browser and use the PromQL expression\n",
    "rate(hello_worlds_total[1m]) to see how many Hello World requests are happening\n",
    "per second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![counter](assets/counter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_http_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_http_server.py\n",
    "import random\n",
    "import http.server\n",
    "from prometheus_client import start_http_server, Counter\n",
    "\n",
    "# Track the number of Hello Worlds returned\n",
    "REQUESTS = Counter('hello_worlds_total', 'Hello Worlds requested.')\n",
    "EXCEPTIONS = Counter('hello_worlds_exceptions_total', 'Exceptions serving Hello World.')\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def do_GET(self):\n",
    "        REQUESTS.inc()\n",
    "        with EXCEPTIONS.count_exceptions():\n",
    "            if random.random() < 0.2:\n",
    "                raise Exception\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![counter-exc](assets/counter_exc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gauge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gauges are a snapshot of some current state. \n",
    "- While for counters how fast it is increasing is what you care about, for gauges it is the actual value of the gauge.\n",
    "- Accordingly, the values can go both up and down. \n",
    "- Examples of gauges include:\n",
    "    - the number of items in a queue\n",
    "    - memory usage of a cache\n",
    "    - number of active threads\n",
    "    - the last time a record was processed\n",
    "    - average requests per second in the last minute. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gauges\n",
    "\n",
    "- Gauges have three main methods you can use:\n",
    "    - inc\n",
    "    - dec\n",
    "    - set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_http_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_http_server.py\n",
    "import random\n",
    "import http.server\n",
    "from prometheus_client import start_http_server, Counter, Gauge\n",
    "import time\n",
    "\n",
    "# Track the number of Hello Worlds returned\n",
    "REQUESTS = Counter('hello_worlds_total', 'Hello Worlds requested.')\n",
    "EXCEPTIONS = Counter('hello_worlds_exceptions_total', 'Exceptions serving Hello World.')\n",
    "\n",
    "# Track the number of calls in progress and when the last oen was completed\n",
    "INPROGRESS = Gauge('hello_worlds_inprogress', 'Number of Hello Worlds in Progress.')\n",
    "LAST = Gauge('hello_world_last_time_seconds', 'The last time a Hello World was served.')\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def do_GET(self):\n",
    "        REQUESTS.inc()\n",
    "        INPROGRESS.inc()\n",
    "        with EXCEPTIONS.count_exceptions():\n",
    "            if random.random() < 0.2:\n",
    "                raise Exception\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "        LAST.set(time.time())\n",
    "        INPROGRESS.dec()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When was the last request made?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/last.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many seconds it is since the last request?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/seconds-since-last.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting simple_http_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_http_server.py\n",
    "import http.server\n",
    "from prometheus_client import start_http_server, Gauge\n",
    "import time\n",
    "\n",
    "\n",
    "# Track the number of calls in progress and when the last oen was completed\n",
    "INPROGRESS = Gauge('hello_worlds_inprogress', 'Number of Hello Worlds in Progress.')\n",
    "LAST = Gauge('hello_world_last_time_seconds', 'The last time a Hello World was served.')\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    @INPROGRESS.track_inprogress()\n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "        LAST.set_to_current_time()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric Suffixes\n",
    "\n",
    "- The example counter metrics all ended with_total, while there is no such suffix on gauges. \n",
    "- This is a convention within Prometheus that makes it easier to identify what type of metric you are working with.\n",
    "- In addition to _total, the _count, _sum, and _bucket suffixes also have other meanings and should not be used as suffixes in your metric names to avoid confusion.\n",
    "- It is also strongly recommended that you include the unit of your metric at the end of its name. For example, a counter for bytes processed might be myapp_requests_processed_bytes_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To track the size or number of items in a cache, you should generally add inc and dec calls in each function where items are added or removed from the cache.\n",
    "- In Python, gauges have a set_function method, which allows you to specify a function to be called at exposition time. \n",
    "- Your function should return a floating point value for the metric when called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing simple_callback.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile simple_callback.py\n",
    "import time\n",
    "from prometheus_client import Gauge\n",
    "\n",
    "TIME = Gauge('time_seconds', 'The current time.')\n",
    "TIME.set_function(lambda: time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Knowing how long your application took to respond to a request or the latency of a backend are vital metrics when you are trying to understand the performance of your systems. \n",
    "- The primary method of a summary is **observe**, to which you pass the size of the event. This must be a nonnegative value.\n",
    "- Using `time.time()` you can track latency as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing summary.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summary.py\n",
    "import http.server\n",
    "from prometheus_client import start_http_server, Summary\n",
    "import time\n",
    "\n",
    "LATENCY = Summary('hello_world_latency_seconds', 'Time for a request Hello World.')\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    def do_GET(self):\n",
    "        start = time.time()\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "        LATENCY.observe(time.time() - start)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/latency_sum.png)\n",
    "![](assets/latency_rate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing summary_decorator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile summary_decorator.py\n",
    "import http.server\n",
    "from prometheus_client import start_http_server, Summary\n",
    "import time\n",
    "\n",
    "LATENCY = Summary('hello_world_latency_seconds', 'Time for a request Hello World.')\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    @LATENCY.time()\n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A summary will provide the average latency, but what if you want a quantile?\n",
    "- Quantiles tell you that a certain proportion of events had a size below a given value.\n",
    "- For example, the 0.95 quantile being 300 ms means that 95% of requests took less than 300 ms. \n",
    "- Quantiles are useful when reasing about actual end-user experience. If a user's browser makes 20 concurrent requests to your application, then it is the slowest of them that determines th user-visible latency. In this case the 95th percentile captures that latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing histogram.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile histogram.py\n",
    "import http.server\n",
    "from prometheus_client import start_http_server, Histogram\n",
    "import time\n",
    "\n",
    "LATENCY = Histogram('hello_world_latency_seconds', 'Time for a request Hello World.')\n",
    "\n",
    "class MyHandler(http.server.BaseHTTPRequestHandler):\n",
    "    @LATENCY.time()\n",
    "    def do_GET(self):\n",
    "        self.send_response(200)\n",
    "        self.end_headers()\n",
    "        self.wfile.write(b\"Hello World\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_http_server(8000)\n",
    "    server = http.server.HTTPServer(('localhost', 8001), MyHandler)\n",
    "    server.serve_forever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaching Instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Should I Instrument?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Service Instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of services:\n",
    "- Online-serving systems\n",
    "- Offline-serving systems\n",
    "- Batch jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Online-serving systems**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Either a human or another service is waiting on a response. These include web servers and databases. \n",
    "- Key metrics to include in service instrumentation:\n",
    "    - request rate\n",
    "    - latency\n",
    "    - error rate\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Offline-serving systems**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do not have someone waiting on them\n",
    "- Usually batch up work and have multiple stages in a pipeline with queues between them\n",
    "- A log processing system is an example of an offline-serving system\n",
    "- For each stage, you should have metrics for \n",
    "  - the amount of queued work, \n",
    "  - how much work is in progress\n",
    "  - how fast you are processing items, and errors that occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Batch Jobs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to offline-serving systems\n",
    "- However, batch jobs run on a regular schedule, whereas offline-serving systems run continuously\n",
    "- As batch jobs are not always running, scraping them doesn't work too well, so techniques such as the Pushgateway are used\n",
    "- At the end of a batch job you should record:\n",
    "    - How long it took to run\n",
    "    - How long each stage of the job took\n",
    "    - The time at which the job last succeeded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thread and worker pools should be instrumented similarly to offline-serving systems.\n",
    "You will want to have metrics for the queue size, active threads, any limit on\n",
    "the number of threads, and errors encountered.\n",
    "- Background maintenance tasks that run no more than a few times an hour are effectively\n",
    "batch jobs, and you should have similar metrics for these tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
